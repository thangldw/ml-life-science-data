{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BLN1vs0waPMq"
      },
      "source": [
        "# 第10章　実践編5：タンパク質の「言語」の法則を解き明かす\n",
        "\n",
        "- 清水 秀幸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dk6IC_IahBP"
      },
      "source": [
        "##### 入力10-1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNHks8o8aopG"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdO9E6FBakJA"
      },
      "source": [
        "##### 入力10-2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIsOMMujatSQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4qoYyRMavEx"
      },
      "source": [
        "##### 入力10-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX82Tn8Ea0bX"
      },
      "outputs": [],
      "source": [
        "!wget https://services.healthtech.dtu.dk/services/DeepLoc-1.0/deeploc_data.fasta -P ./data -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeJ5JOypakL-"
      },
      "source": [
        "##### 入力10-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-DAlwrNa3Fo"
      },
      "outputs": [],
      "source": [
        "!ls ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr2-5LDlakO9"
      },
      "source": [
        "##### 入力10-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkWchC8wa6GX"
      },
      "outputs": [],
      "source": [
        "!head -n 6 ./data/deeploc_data.fasta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMGl0FPEakTl"
      },
      "source": [
        "##### 入力10-6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5DkugWWa9rx"
      },
      "outputs": [],
      "source": [
        "!wc -l ./data/deeploc_data.fasta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeeneb91akWT"
      },
      "source": [
        "##### 入力10-7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3CR8s9dbEYt"
      },
      "outputs": [],
      "source": [
        "!pip install Bio -q\n",
        "import Bio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGMMT_TxakY4"
      },
      "source": [
        "##### 入力10-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY9gfmzxbIOP"
      },
      "outputs": [],
      "source": [
        "def read_fasta(file_path, columns) :\n",
        "    from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "    with open(file_path) as fasta_file:\n",
        "        records = []\n",
        "        for title, sequence in SimpleFastaParser(fasta_file):\n",
        "            record = []\n",
        "            title_splits = title.split(None)\n",
        "            record.append(title_splits[0])\n",
        "            sequence = \"\".join(sequence)\n",
        "            record.append(sequence)\n",
        "            record.append(len(sequence))\n",
        "            location_splits = title_splits[1].split(\"-\")\n",
        "            record.append(location_splits[0])\n",
        "            record.append(location_splits[1])\n",
        "\n",
        "            if(len(title_splits) > 2):\n",
        "                record.append(0)\n",
        "            else:\n",
        "                record.append(1)\n",
        "                \n",
        "            records.append(record)\n",
        "    return pd.DataFrame(records, columns = columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYt6w8-AakcD"
      },
      "source": [
        "##### 入力10-9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMs4gYAbbb5g"
      },
      "outputs": [],
      "source": [
        "data = read_fasta('./data/deeploc_data.fasta', columns=['id', 'sequence', 'sequence_length', 'location', 'membrane', 'is_train'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1xdH4uake9"
      },
      "source": [
        "##### 入力10-10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFL-NmBybgb1"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swaX1Pn-akmm"
      },
      "source": [
        "##### 入力10-11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdRR3WebjQK"
      },
      "outputs": [],
      "source": [
        "data['sequence_length'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyCiKDOxakoe"
      },
      "source": [
        "##### 入力10-12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9hWBvGVbm9S"
      },
      "outputs": [],
      "source": [
        "data = data[data['sequence_length'] < 1000]\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJIkYZx8akqF"
      },
      "source": [
        "##### 入力10-13\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcittGEDbqtk"
      },
      "outputs": [],
      "source": [
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "ax = sns.histplot(data['sequence_length'].values)\n",
        "ax.set_xlim(0, 1000)\n",
        "plt.title(f'sequence length distribution')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmLlhf-akrl"
      },
      "source": [
        "##### 入力10-14\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_dencH0bvsc"
      },
      "outputs": [],
      "source": [
        "data.isnull().values.any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JVxaDW0aktO"
      },
      "source": [
        "##### 入力10-15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t0Q_486byeb"
      },
      "outputs": [],
      "source": [
        "unique_classes = data.location.unique()\n",
        "print('Number of classes: ', len(unique_classes))\n",
        "print(unique_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcYiRCs4akur"
      },
      "source": [
        "##### 入力10-16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO3-Q4veb7Tc"
      },
      "outputs": [],
      "source": [
        "categories = data.location.astype('category').cat\n",
        "data['location'] = categories.codes\n",
        "class_names = categories.categories\n",
        "num_classes = len(class_names)\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hoW9IyGakwt"
      },
      "source": [
        "##### 入力10-17\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mCFbGZqcA5e"
      },
      "outputs": [],
      "source": [
        "data['location']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyuCgtGFaky_"
      },
      "source": [
        "##### 入力10-18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc0p9Ub6cF38"
      },
      "outputs": [],
      "source": [
        "df_train = data[data.is_train == 1]\n",
        "df_train = df_train.drop(['is_train'], axis = 1)\n",
        "print(df_train.shape[0])\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eudJzetak0G"
      },
      "source": [
        "##### 入力10-19\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFnFRY4NcJwU"
      },
      "outputs": [],
      "source": [
        "df_test = data[data.is_train == 0]\n",
        "df_test = df_test.drop(['is_train'], axis = 1)\n",
        "print(df_test.shape[0])\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLigZVHhak3B"
      },
      "source": [
        "##### 入力10-20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VRYAXTecN6m"
      },
      "outputs": [],
      "source": [
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIYjv020ak39"
      },
      "source": [
        "##### 入力10-21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEP-tm7VcRj-"
      },
      "outputs": [],
      "source": [
        "def get_T5_model():\n",
        "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc')\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS0Tjsc7ak48"
      },
      "source": [
        "##### 入力10-22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjiS_Jodcgbm"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = get_T5_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MocYtRFcak8L"
      },
      "source": [
        "##### 入力10-23\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grGsMkqycjie"
      },
      "outputs": [],
      "source": [
        "train_sequences = { i: seq for i, seq in enumerate(df_train['sequence']) }\n",
        "train_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhUZBgRSak8_"
      },
      "source": [
        "##### 入力10-24\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rKuKPvGcvvP"
      },
      "outputs": [],
      "source": [
        "len(train_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcWK-m7Sak97"
      },
      "source": [
        "##### 入力10-25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SocPjjFczHP"
      },
      "outputs": [],
      "source": [
        "test_sequences = { i: seq.replace(' ', '') for i, seq in enumerate(df_test['sequence']) }\n",
        "len(test_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNuc_dvSalBj"
      },
      "source": [
        "##### 入力10-26\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj117NOpc1wR"
      },
      "outputs": [],
      "source": [
        "def get_embeddings( model, tokenizer, seqs, max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
        "    results = {'protein_embs' : dict()}\n",
        "    seq_dict = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
        "    start = time.time()\n",
        "    batch = list()\n",
        "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
        "        seq = seq\n",
        "        seq_len = len(seq)\n",
        "        seq = ' '.join(list(seq))\n",
        "        batch.append((pdb_id,seq,seq_len))\n",
        "\n",
        "        n_res_batch = sum([ s_len for _, _, s_len in batch ]) + seq_len\n",
        "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
        "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
        "            batch = list()\n",
        "\n",
        "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding='longest')\n",
        "            input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
        "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
        "            except RuntimeError:\n",
        "                print('RuntimeError during embedding for {} (L={})'.format(pdb_id, seq_len))\n",
        "                continue\n",
        "            \n",
        "            for batch_idx, identifier in enumerate(pdb_ids):\n",
        "                s_len = seq_lens[batch_idx]\n",
        "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
        "                protein_emb = emb.mean(dim=0)\n",
        "                results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
        "\n",
        "\n",
        "    passed_time=time.time()-start\n",
        "    avg_time = passed_time/len(results['protein_embs'])\n",
        "    print('\\n##### EMBEDDING COMPLETED ######')\n",
        "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
        "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
        "        passed_time/60, avg_time ))\n",
        "    print('\\n############# END #############')\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg4YM4dRalCs"
      },
      "source": [
        "##### 入力10-27\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZgLsqYTdtgy"
      },
      "outputs": [],
      "source": [
        "train_embeddings = get_embeddings(model, tokenizer, train_sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpHsPtoUalDu"
      },
      "source": [
        "##### 入力10-28\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OROOMN4QdwU8"
      },
      "outputs": [],
      "source": [
        "print(train_embeddings['protein_embs'][0])\n",
        "print(train_embeddings['protein_embs'][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCE_JfJzalE_"
      },
      "source": [
        "##### 入力10-29\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeX6o81ieVJ1"
      },
      "outputs": [],
      "source": [
        "test_embeddings = get_embeddings(model, tokenizer, test_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juHjBUwTalIS"
      },
      "source": [
        "##### 入力10-30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXZAzW09eX5o"
      },
      "outputs": [],
      "source": [
        "# 目的変数(局在情報)と入力変数(タンパク質の1,024次元の数値ベクトル)をまとめてデータセットに変換\n",
        "train_embedding_matrices = torch.zeros(len(df_train), 1024)\n",
        "for i, v in enumerate(train_embeddings['protein_embs'].values()):\n",
        "    train_embedding_matrices[i] = torch.from_numpy(v.astype(np.float32))\n",
        "target = torch.tensor(df_train['location'].values, dtype=torch.int64)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_embedding_matrices, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l2O4TjYalKe"
      },
      "source": [
        "##### 入力10-31\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPZS2kXfeeHd"
      },
      "outputs": [],
      "source": [
        "#バッチサイズ\n",
        "batch_size = 64\n",
        "\n",
        "# shuffle はデフォルトで False のため，学習データのみ True に指定\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKQMhk8alLv"
      },
      "source": [
        "##### 入力10-32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fLo9wTJeg8e"
      },
      "outputs": [],
      "source": [
        "# ニューラルネットワークの定義\n",
        "\n",
        "class Simple_Net(nn.Module):\n",
        "    # 使用するオブジェクトを定義\n",
        "    def __init__(self):\n",
        "        super(Simple_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(1024, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    # 順伝播. 活性化関数を明示して表記している\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "# インスタンス化\n",
        "simple_net = Simple_Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzxsCIqalOj"
      },
      "source": [
        "##### 入力10-33\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K7rLPhyerp2"
      },
      "outputs": [],
      "source": [
        "# 損失関数・最適化の設定\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(simple_net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBD0JaG8alRI"
      },
      "source": [
        "##### 入力10-34\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XchD_AxZeu3H"
      },
      "outputs": [],
      "source": [
        "# 100エポック学習\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        # 学習ステップ\n",
        "        optimizer.zero_grad()\n",
        "        outputs = simple_net(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    loss_history.append(total_loss)\n",
        "    print(epoch + 1, total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoSN7adalTy"
      },
      "source": [
        "##### 入力10-35\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C44g3IfefxRz"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFtaqPFLalUy"
      },
      "source": [
        "##### 入力10-36\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqnxrw31f1Kv"
      },
      "outputs": [],
      "source": [
        "# 目的変数(局在情報)と入力変数(タンパク質の1,024次元の数値ベクトル)をまとめてデータセットに変換\n",
        "test_embedding_matrices = torch.zeros(len(df_test), 1024)\n",
        "\n",
        "for i, v in enumerate(test_embeddings['protein_embs'].values()):\n",
        "    test_embedding_matrices[i] = torch.from_numpy(v.astype(np.float32))\n",
        "target = torch.tensor(df_test['location'].values, dtype=torch.int64)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_embedding_matrices, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw8EJpHKalW7"
      },
      "source": [
        "##### 入力10-37\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gizBYMsnf6yS"
      },
      "outputs": [],
      "source": [
        "# バッチサイズ\n",
        "batch_size = 64\n",
        "\n",
        "# shuffle はデフォルトで False\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aboUOItOalYs"
      },
      "source": [
        "##### 入力10-38\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8OvvDEEf_OR"
      },
      "outputs": [],
      "source": [
        "## テストデータにおける正解率を検証\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        outputs = simple_net(x)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "print('正解率', int(correct)/total*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5-Uix78ala4"
      },
      "source": [
        "##### 入力10-39\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttIsbXP6gOnS"
      },
      "outputs": [],
      "source": [
        "true_list = []\n",
        "pred_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        outputs = simple_net(x)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        pred_list += predicted.detach().numpy().tolist()\n",
        "        true_list += y.detach().numpy().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjy25XHalbw"
      },
      "source": [
        "##### 入力10-40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAVS2CgjgUR9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(true_list, pred_list)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxg7mxg3aldA"
      },
      "source": [
        "##### 入力10-41\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJk13zWJgXzr"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMpvwpialdv"
      },
      "source": [
        "##### 入力10-42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYw87PDgarM"
      },
      "outputs": [],
      "source": [
        "cm = pd.DataFrame(data=cm, index=class_names.tolist(),\n",
        "                                              columns=class_names.tolist())\n",
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues', fmt='d')\n",
        "plt.yticks(rotation=0)\n",
        "plt.xlabel(\"Prediction\", fontsize=13, rotation=0)\n",
        "plt.ylabel(\"Ground Truth\", fontsize=13)\n",
        "ax.set_ylim(len(cm), 0)\n",
        "print('アミノ酸配列のみからタンパク質局在を予測した結果')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
