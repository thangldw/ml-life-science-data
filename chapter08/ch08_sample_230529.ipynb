{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-U8JXijLwXG"
      },
      "source": [
        "# 第8章　実践編3: シングルセル解析とVAE\n",
        "\n",
        "- 水越周良\n",
        "- 小嶋泰弘\n",
        "- 島村徹平"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "編集部注：2023年5月29日最終更新．コードの一部がお手元の書籍と異なる可能性がございます．正誤・更新情報は弊社ウェブサイトの[本書詳細ページ](https://www.yodosha.co.jp/jikkenigaku/book/9784758122634/index.html)をご参照ください．"
      ],
      "metadata": {
        "id": "9zTtrdDMX3X_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuR9IaPvNA4u"
      },
      "source": [
        "##### 入力8-1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwETR8wnNMqE"
      },
      "outputs": [],
      "source": [
        "!pip install scanpy==1.9.3 umap-learn==0.5.3 leidenalg==0.9.1 pyro-ppl==1.8.4 scvi==0.6.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLg-AMIONIvp"
      },
      "source": [
        "##### 入力8-2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWC6lA2ENQ2N"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scanpy as sc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch import functional as F\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn import init\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy\n",
        "import umap\n",
        "import anndata as ad\n",
        "import sklearn.decomposition\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "import pyro\n",
        "import scvi\n",
        "import scvi.dataset\n",
        "from scvi.dataset import CortexDataset # RetinaDatasetやPbmcDatasetにもクラスタのラベルがある"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71G-YhYENIzE"
      },
      "source": [
        "##### 入力8-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHJqw_6ANVKW"
      },
      "outputs": [],
      "source": [
        "import warnings; warnings.simplefilter('ignore') # 警告メッセージを表示しない\n",
        "cortex = CortexDataset(save_path='data/', total_genes=None) # scVIのデータセットをロードする\n",
        "n_genes = 1000 # VAEに入れる遺伝子の数を設定する. 多くの細胞を含むデータの際はより多くの遺伝子を用いたほうが良い\n",
        "cortex.subsample_genes(n_genes, mode=\"variance\") # 細胞間の分散が高い順から遺伝子を選ぶ\n",
        "adata = ad.AnnData(cortex.X) # anndataにデータを格納\n",
        "adata.var_names = cortex.gene_names # 遺伝子名を格納\n",
        "adata.obs_names = [f'{i}' for i in range (adata.n_obs)]\n",
        "adata.obs['cluster_labels'] = cortex.labels.astype('str') # データセットに付属のクラスタラベルを格納\n",
        "adata.layers['raw_counts'] = adata.X.astype(int) # VAEのモデルでは正規化前の発現量を用いるために別の場所に格納する\n",
        "sc.pp.filter_cells(adata, min_genes=200) # 発現する遺伝子が少なすぎる細胞を除く\n",
        "sc.pp.normalize_total(adata, target_sum=1e4) # 正規化\n",
        "sc.pp.log1p(adata) # 対数変換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHgae4E6NI2Q"
      },
      "source": [
        "##### 入力8-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnUvSSHDNZzO"
      },
      "outputs": [],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llbp8kt3NI40"
      },
      "source": [
        "##### 入力8-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRX2FYwqN5Q4"
      },
      "outputs": [],
      "source": [
        "class LinearReLU(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearReLU, self).__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim), # 線形変換\n",
        "            nn.BatchNorm1d(output_dim), # バッチ正規化\n",
        "            nn.ReLU(True)) # 活性化関数\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.f(x)\n",
        "        return(h)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim, h_dim, n_genes):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.x2h = LinearReLU(n_genes, h_dim)\n",
        "        self.seq_nn = LinearReLU(h_dim, h_dim)\n",
        "        self.h2mu = nn.Linear(h_dim, z_dim) # zの従う正規分布の平均を出力\n",
        "        self.h2logvar = nn.Linear(h_dim, z_dim) # zの従う正規分布の分散を出力\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_h = self.x2h(x)\n",
        "        post_h = self.seq_nn(pre_h)\n",
        "        mu = self.h2mu(post_h)\n",
        "        logvar = self.h2logvar(post_h)\n",
        "        return(mu, logvar)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, h_dim, n_genes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.z2h = LinearReLU(z_dim, h_dim)\n",
        "        self.seq_nn = LinearReLU(h_dim, h_dim)\n",
        "        self.h2ld = nn.Linear(h_dim, n_genes)\n",
        "        self.softplus = nn.Softplus()\n",
        "    \n",
        "    def forward(self, z):\n",
        "        pre_h = self.z2h(z)\n",
        "        post_h = self.seq_nn(pre_h)\n",
        "        ld = self.h2ld(post_h)\n",
        "        correct_ld = self.softplus(ld) # 出力が正の値(特に0から1の間)になるようにsoftplus変換する\n",
        "        return(correct_ld)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefklAvpNI7V"
      },
      "source": [
        "##### 入力8-6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhqn3YeZN7tZ"
      },
      "outputs": [],
      "source": [
        "def calcNbLoss(ld, norm_mat, r, obs): # デコーダの分布が負の二項分布に従うと仮定した場合の対数尤度を求める\n",
        "    ld = norm_mat * ld\n",
        "    p = ld / (ld + r)\n",
        "    p_z = dist.NegativeBinomial(r, p)\n",
        "    l = - p_z.log_prob(obs) # 対数尤度を求める\n",
        "    return(l)\n",
        "\n",
        "def calcZeroInflatedNbLoss(ld, norm_mat, r, obs, gate): # デコーダの分布がゼロ過剰負の二項分布に従うと仮定した場合の対数尤度を求める\n",
        "    ld = norm_mat * ld\n",
        "    p = ld / (ld + r)\n",
        "    p_z = pyro.distributions.zero_inflated.ZeroInflatedDistribution(base_dist=dist. NegativeBinomial(r, p),gate=gate)\n",
        "    l = - p_z.log_prob(obs) # 対数尤度を求める\n",
        "    return(l)\n",
        "\n",
        "def calcPoissonLoss(ld, norm_mat, obs): # デコーダの分布がポアソン分布に従うと仮定した場合の対数尤度を求める\n",
        "    p_z = dist.Poisson(ld * norm_mat)\n",
        "    l = - p_z.log_prob(obs) # 対数尤度を求める\n",
        "    return(l)\n",
        "\n",
        "def calcZeroInflatedPoissonLoss(ld, norm_mat, obs, gate): # デコーダの分布がゼロ過剰ポアソン分布に従うと仮定した場合の対数尤度を求める\n",
        "    p_z = pyro.distributions.zero_inflated.ZeroInflatedDistribution(base_dist=dist.Poisson(ld * norm_mat), gate=gate)\n",
        "    l = - p_z.log_prob(obs) # 対数尤度を求める\n",
        "    return(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIywxaJ8NI_G"
      },
      "source": [
        "##### 入力8-7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtN2EcJzO0ob"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim, h_dim, n_genes, likelihood_function='zero-inflated_negative_binominal'):\n",
        "        super(VAE, self).__init__()\n",
        "        self.r = Parameter(torch.Tensor(n_genes)) # 負の二項分布に必要なパラメータ\n",
        "        self.drop_rate = Parameter(torch.Tensor(n_genes)) # ゼロ過剰分布に必要なパラメータ\n",
        "        self.enc_z = Encoder(z_dim, h_dim, n_genes) # エンコーダをインスタンス化\n",
        "        self.dec_z = Decoder(z_dim, h_dim, n_genes) # デコーダをインスタンス化\n",
        "        self.softplus = nn.Softplus() # softplus関数. 値を正の値に変換する. ReLUと比べ，入力が負の値でもパラメータの更新が行われるメリットがある\n",
        "        self.sigmoid = nn.Sigmoid() # シグモイド関数. 値を0から1の間の値に変換する\n",
        "        self.likelihood_function = likelihood_function\n",
        "        self.reset_parameters() # 学習開始時にパラメータをリセットする\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.r)\n",
        "        init.normal_(self.drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qz_mu, qz_logvar = self.enc_z(x) # エンコーダからzの平均と分散を出力\n",
        "        qz = dist.Normal(qz_mu, self.softplus(qz_logvar)) # zの従う正規分布を作る\n",
        "        z = qz.rsample() # reparameterization trickにより逆伝播できるようになる\n",
        "        x_hat = self.dec_z(z) # デコーダからの出力\n",
        "        return(qz, x_hat)\n",
        "\n",
        "    def elbo_loss(self, x, norm_mat):\n",
        "        qz, x_hat = self(x)\n",
        "        kld = 0.5 * (qz.loc.pow(2) + qz.scale.pow(2) - 1 - qz.scale.pow(2).log()) # KLダイバ ージェンス\n",
        "        if self.likelihood_function == 'zero-inflated_negative_binominal':\n",
        "        # rは負の二項分布における失敗の回数と同義であり，ポアソン分布からの離れ具合を制御する. rが無限大の時，負の二項分布はポアソン分布と一致する\n",
        "            r = self.softplus(self.r) # 0より大きい値を取るために，softplusで変換する\n",
        "            drop_rate = self.sigmoid(self.drop_rate) # ゼロ過剰分布において，サンプリングされた値が0になる割合. シグモイド関数で0から1の値にする\n",
        "            lx = calcZeroInflatedNbLoss(x_hat, norm_mat, r, x, drop_rate) # デコーダの分布の対数尤度を求める\n",
        "        if self.likelihood_function == 'negative_binominal':\n",
        "            r = self.softplus(self.r)\n",
        "            lx = calcNbLoss(x_hat, norm_mat, r, x)\n",
        "        if self.likelihood_function == 'zero-inflated_poisson':\n",
        "            drop_rate = self.sigmoid(self.drop_rate)\n",
        "            lx = calcZeroInflatedPoissonLoss(x_hat, norm_mat, x, drop_rate)\n",
        "        if self.likelihood_function == 'poisson':\n",
        "            lx = calcPoissonLoss(x_hat, norm_mat, x)\n",
        "        elbo_loss = torch.sum((torch.sum(kld, dim=-1))) + torch.sum((torch.sum(lx, dim=-1)))\n",
        "        return(elbo_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkw05QqoNJBn"
      },
      "source": [
        "##### 入力8-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YszN4mDO6Nz"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, path='checkpoint.pt'): # 何回更新が止まったら学習を止めるのか設定\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.stop_flag = False # これがTrueになったら学習を止める\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, validation_loss, model):\n",
        "        score = validation_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score # 1エポック目はそのままスコアを保存\n",
        "            self.checkpoint(validation_loss, model)\n",
        "        elif score > self.best_score: # 損失が更新されなかったらカウンタに1を加える\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience: # 設定カウントになったら学習を止める\n",
        "                self.stop_flag = True\n",
        "        else: # 損失が減った場合\n",
        "            self.best_score = score # その損失をベストスコアにする\n",
        "            self.checkpoint(validation_loss, model) # モデルを保存\n",
        "            self.counter = 0 # カウンタをリセット\n",
        "\n",
        "    def checkpoint(self, validation_loss, model):\n",
        "        torch.save(model.state_dict(), self.path) # パラメータをpathに保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-o1K9TvNJEL"
      },
      "source": [
        "##### 入力8-9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qv3sDjgPRnm"
      },
      "outputs": [],
      "source": [
        "class DataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, norm_mat, transform=None, pre_transform=None):\n",
        "        self.x = x\n",
        "        self.norm_mat = norm_mat\n",
        "\n",
        "    def __len__(self):\n",
        "        return(self.x.shape[0]) # 全細胞数を返す\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        idx_x = self.x[idx]\n",
        "        idx_norm_mat = self.norm_mat[idx]\n",
        "        return(idx_x, idx_norm_mat) # 与えられたindexに対応するxとnorm_matを返す\n",
        "\n",
        "class DataManager():\n",
        "    def __init__(self, x, test_ratio=0.05, batch_size=100, num_workers=1, validation_ratio=0.1):\n",
        "        x = x.float()\n",
        "        # デコーダからの出力の平均が1になるような行列を作る\n",
        "        norm_mat = torch.sum(x, dim=1).view(-1, 1) * torch.sum(x, dim=0).view(1, -1)\n",
        "        norm_mat = torch.mean(x) * norm_mat / torch.mean(norm_mat)\n",
        "        self.x = x\n",
        "        self.norm_mat = norm_mat\n",
        "        total_num = x.shape[0] # 全細胞数\n",
        "        validation_num = int(total_num * validation_ratio) # validationに使う細胞数\n",
        "        test_num = int(total_num * test_ratio) # testに使う細胞数\n",
        "        np.random.seed(42) # シード値を指定\n",
        "        idx = np.random.permutation(np.arange(total_num)) # 細胞のインデックスをシャッフルし， validation，test，trainingに使われる細胞のインデックスを得る\n",
        "        validation_idx, test_idx, train_idx = idx[:validation_num], idx[validation_num:(validation_num + test_num)], idx[(validation_num + test_num):]\n",
        "        self.validation_idx, self.test_idx, self.train_idx = validation_idx, test_idx, train_idx\n",
        "        self.validation_x = x[validation_idx]\n",
        "        self.validation_norm_mat = norm_mat[validation_idx]\n",
        "        self.test_x = x[test_idx]\n",
        "        self.test_norm_mat = norm_mat[test_idx]\n",
        "        self.train_eds = DataSet(x[train_idx], norm_mat[train_idx]) # trainingに用いる細胞を格納\n",
        "        # DataLoaderはDataSetから受け取ったデータをミニバッチごと(今回は100細胞ごと)に分けて出力してくれる\n",
        "        self.train_loader = torch.utils.data.DataLoader(self.train_eds, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405NZ2ShNJGW"
      },
      "source": [
        "##### 入力8-10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MXsV5NQQ8ir"
      },
      "outputs": [],
      "source": [
        "class Experiment:\n",
        "    def __init__(self, lr, x, z_dim, h_dim,n_genes, likelihood_function):\n",
        "        self.dm = DataManager(x) # DataManagerクラスをインスタンス化\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 可能ならGPUを使用\n",
        "        self.model = VAE(z_dim, h_dim, n_genes, likelihood_function) # VAEクラスをインスタンス化\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) # OptimizerにはAdamを使用\n",
        "        self.train_loss_list = [] # training lossを記録する\n",
        "        self.val_loss_list = [] # validation lossを記録する\n",
        "\n",
        "    def train_epoch(self): # trainingの実行\n",
        "        self.model.train() # trainingモードにする\n",
        "        total_loss = 0\n",
        "        entry_num = 0\n",
        "        for x, norm_mat in self.dm.train_loader: # 学習用データをバッチごとに出力してくれる\n",
        "            x = x.to(self.device) # 100細胞の発現量が取り出される\n",
        "            norm_mat = norm_mat.to(self.device) # 取り出された細胞に対応するnorm_mat\n",
        "            self.optimizer.zero_grad() # 前回計算した勾配がoptimizerに残っているため初期化する\n",
        "            loss = self.model.elbo_loss(x, norm_mat) # ELBOを計算する\n",
        "            loss.backward() # 誤差逆伝播を行う\n",
        "            self.optimizer.step() # パラメータの最適化を行う\n",
        "            total_loss += loss.item() # バッチごとにlossが計算されるのでそれを足していく\n",
        "            entry_num += x.shape[0] # バッチごとに細胞数を足していく\n",
        "        return(total_loss / entry_num) # すべてのバッチでのlossの合計を細胞数で割ったものが出力される\n",
        "    \n",
        "    def evaluate(self): # validation lossを計算\n",
        "        self.model.eval() # validationとtest時にはこのように書く\n",
        "        x = self.dm.validation_x.to(self.device) # validation用のxを取り出す\n",
        "        norm_mat = self.dm.validation_norm_mat.to(self.device) # 取り出したxに対応するnorm_mat\n",
        "        \n",
        "        loss = self.model.elbo_loss(x, norm_mat)\n",
        "        entry_num = x.shape[0]\n",
        "        loss_val = loss / entry_num\n",
        "        return(loss_val) # lossの値を出力\n",
        "    \n",
        "    def test(self): # test_lossを計算\n",
        "        self.model.eval()\n",
        "        x = self.dm.test_x.to(self.device) # test用のxを取り出す\n",
        "        norm_mat = self.dm.test_norm_mat.to(self.device) # 取り出したxに対応するnorm_mat\n",
        "        loss = self.model.elbo_loss(x, norm_mat)\n",
        "        entry_num = x.shape[0]\n",
        "        loss_val = loss / entry_num\n",
        "        return(loss_val)\n",
        "    \n",
        "    def train_total(self, epoch_num): # trainingの回数や終了を管理する\n",
        "        earlystopping = EarlyStopping() # EarlyStoppingクラスをインスタンス化する\n",
        "        for epoch in range(epoch_num): # 指定された回数，学習を行う\n",
        "            loss = self.train_epoch() # trainingを行うtrain_epoch()を呼び出す\n",
        "            val_loss = self.evaluate() # validation lossの計算\n",
        "            self.train_loss_list.append(loss) # listにtraining lossを格納\n",
        "            self.val_loss_list.append(val_loss) # listにvalidation lossを格納\n",
        "            if epoch % 20 == 0:\n",
        "                print(f'validation loss at epoch {epoch} is {val_loss:.2f}') # 20回ごとに validation lossを出力\n",
        "            earlystopping(val_loss, self.model) # validation lossの増減を判断\n",
        "            if earlystopping.stop_flag: # ストップフラグがTrueの場合，breakでtrainingを終える\n",
        "                print(f'Early Stopping at {epoch} epoch')\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUQ1rIZTNJI3"
      },
      "source": [
        "##### 入力8-11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sozsqew7RQqj"
      },
      "outputs": [],
      "source": [
        "lr = 0.001 # 学習率を設定\n",
        "if type(adata.layers['raw_counts']) == np.ndarray: # 入力をndarrayからtensorに変換\n",
        "    x = torch.tensor(adata.layers['raw_counts'].copy())\n",
        "else: # 疎行列の場合はndarrayに変換してから，tensorに変換\n",
        "    x = torch.tensor(adata.layers['raw_counts'].toarray().copy())\n",
        "z_dim = 10 # zの次元数\n",
        "h_dim = 128 # 隠れ層の次元数\n",
        "vae_exp = Experiment(lr, x, z_dim, h_dim, n_genes, likelihood_function='zero-inflated_negative_binominal')\n",
        "print('Start!')\n",
        "vae_exp.train_total(500) # 学習の実行\n",
        "vae_exp.model.load_state_dict(torch.load('checkpoint.pt')) # 最後に保存したモデルを読み込む\n",
        "print(f'Done! validation_loss:{vae_exp.evaluate():.2f}')\n",
        "# 学習曲線を描こう\n",
        "val = [i.item() for i in vae_exp.val_loss_list]\n",
        "train = vae_exp.train_loss_list\n",
        "xx = [i for i in range(len(val))]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "ax.plot(xx, val, label='validation_loss')\n",
        "ax.plot(xx, train, label='training_loss')\n",
        "ax.legend()\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYha6ZDzNJL2"
      },
      "source": [
        "##### 入力8-12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZlg1clkRi4m"
      },
      "outputs": [],
      "source": [
        "x = vae_exp.dm.x # 今回用いた細胞を取り出す\n",
        "qz_mu, qz_logvar = vae_exp.model.enc_z(x) # 学習後のエンコーダから平均と分散を得る\n",
        "qz = dist.Normal(qz_mu, vae_exp.model.softplus(qz_logvar))\n",
        "z_vae = qz.loc # 学習を終えた後は，zをランダムサンプリングで求めるのではなく，平均の値を用いる\n",
        "adata.obsm['z_vae'] = z_vae.to('cpu').detach().numpy().copy()\n",
        "sc.pp.neighbors(adata, n_neighbors=20, n_pcs=z_dim, use_rep='z_vae', key_added='z_vae') # z_vaeの値で近傍グラフを作成\n",
        "sc.tl.umap(adata, neighbors_key='z_vae')\n",
        "sc.tl.leiden(adata, neighbors_key='z_vae', key_added='VAE_clusters') # z_vaeの値でクラスタリング\n",
        "sc.pl.umap(adata, color=['VAE_clusters', 'cluster_labels'], wspace=0.3) # umapで図示\n",
        "\n",
        "train_counts = adata.X[vae_exp.dm.train_idx] # 評価のため，PCAのモデルの学習にもtrainingに含まれる細胞のみを用いる\n",
        "pca_model = sklearn.decomposition.PCA(n_components=z_dim) # zの次元数と同じ次元まで削減\n",
        "pca_model.fit(train_counts) # PCAのモデルの学習\n",
        "z_pca = pca_model.transform(adata.X) # 学習したモデルですべての細胞を次元削減\n",
        "adata.obsm['z_pca'] = z_pca\n",
        "sc.pp.neighbors(adata, n_neighbors=20, n_pcs=z_dim, use_rep='z_pca', key_added='z_pca') # z_pcaの値で近傍グラフを作成\n",
        "sc.tl.umap(adata, neighbors_key='z_pca')\n",
        "sc.tl.leiden(adata, neighbors_key='z_pca', key_added='PCA_clusters') # z_pcaの値でクラスタリング\n",
        "sc.pl.umap(adata, color=['PCA_clusters','cluster_labels'], wspace=0.3)\n",
        "\n",
        "clusters_label = adata.obs['cluster_labels'].values.tolist() # データセットのラベル\n",
        "pca_clusters_label = adata.obs['PCA_clusters'].values.tolist()\n",
        "vae_clusters_label = adata.obs['VAE_clusters'].values.tolist()\n",
        "# ARIで与えられたクラスタとの類似度を見る\n",
        "print('\\n',f'ARI score : clusters_label vs PCA_clusters {adjusted_rand_score(clusters_label, pca_clusters_label):.3f}')\n",
        "print((f'ARI score : clusters_label vs VAE_clusters {adjusted_rand_score(clusters_label, vae_clusters_label):.3f}'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}